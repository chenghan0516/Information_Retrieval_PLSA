{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "query_list_path = \"q_100_d_10000/query_list.txt\"\n",
    "doc_list_path = \"q_100_d_10000/doc_list.txt\"\n",
    "\n",
    "with open(query_list_path, \"r\") as f:\n",
    "    q_list = f.read().split('\\n')[:-1]\n",
    "with open(doc_list_path, \"r\") as f:\n",
    "    d_list = f.read().split('\\n')[:-1]\n",
    "\n",
    "def get_query_word(q):\n",
    "    with open(\"q_100_d_10000/queries/{}.txt\".format(q),'r') as f:\n",
    "        words = f.read().split(' ')\n",
    "    return words\n",
    "def get_doc_word(d):\n",
    "    with open(\"q_100_d_10000/docs/{}.txt\".format(d),'r') as f:\n",
    "        words = f.read().split(' ')\n",
    "    return words\n",
    "def get_random_probability_matrix(event_num,condition_num):\n",
    "    matrix=np.random.random_sample((event_num,condition_num))\n",
    "    for i in range(condition_num):\n",
    "        temp_sum = matrix[:,i].sum()\n",
    "        matrix[:,i]/=temp_sum #set sum to 1\n",
    "    return matrix\n",
    "\n",
    "class parameter_retriever:\n",
    "    def __init__(self,topic_num):\n",
    "        self.create_index_term_set()\n",
    "        self.word_num = len(self.index_term_list)\n",
    "        self.topic_num = topic_num\n",
    "        self.doc_num = len(d_list)\n",
    "        self.get_word_count_in_doc()\n",
    "        self.initPossibilities()\n",
    "\n",
    "\n",
    "    def create_index_term_set(self):\n",
    "        print(\"creating index term set\")\n",
    "        print(\" creating index term set from query\")\n",
    "        index_term_set_q = set()\n",
    "        for q in q_list:\n",
    "            words = get_query_word(q)\n",
    "            index_term_set_q = index_term_set_q.union(set(words))\n",
    "\n",
    "        print(\" creating index term set from doc\")\n",
    "        index_term_set_d = set()\n",
    "        for d in d_list:\n",
    "            words = get_doc_word(d)\n",
    "            index_term_set_d = index_term_set_d.union(set(words))\n",
    "\n",
    "        self.index_term_list = list(index_term_set_q.intersection(index_term_set_d))\n",
    "        print(\"number of words in index_term_set: {}\".format(len(self.index_term_list)))\n",
    "        print(\"...done\")\n",
    "\n",
    "    def get_word_count_in_doc(self):\n",
    "        self.c_wd = np.zeros((self.word_num,self.doc_num))\n",
    "        self.doc_length = np.zeros(self.doc_num)\n",
    "        self.c_w=np.zeros(self.word_num)\n",
    "\n",
    "        for idx_d,d in enumerate(d_list):\n",
    "            words = get_doc_word(d)\n",
    "            for idx_term,term in enumerate(self.index_term_list):\n",
    "                temp_count = words.count(term)\n",
    "                self.c_wd[idx_term,idx_d] = temp_count\n",
    "                self.doc_length[idx_d] += temp_count\n",
    "                self.c_w[idx_term]+=1\n",
    "\n",
    "    def initPossibilities(self,k):\n",
    "        print(\"initializing possibilities\")\n",
    "        self.P_w_T = get_random_probability_matrix(self.word_num,self.topic_num)\n",
    "        self.P_T_d = get_random_probability_matrix(self.topic_num,self.doc_num)\n",
    "        self.P_T_wd = np.zeros((self.topic_num,self.word_num,self.doc_num))\n",
    "        print(\"...done\")\n",
    "\n",
    "    def E_step(self):\n",
    "        print(\"start E_step\")\n",
    "        for i in range(self.word_num):\n",
    "            for j in range(self.doc_num):\n",
    "                for k in range(self.topic_num):\n",
    "                    self.P_T_wd[k,i,j] = self.P_w_T[i,k]*self.P_T_d[k,j]\n",
    "                sum_of_topic_k = self.P_T_wd[:,i,j].sum()\n",
    "                self.P_T_wd[:,i,j] /= sum_of_topic_k\n",
    "        print(\"...done\")\n",
    "\n",
    "    def M_step(self):\n",
    "        print(\"start M_step\")\n",
    "        print(\" process P_w_T\")\n",
    "        for k in range(self.topic_num):\n",
    "            for i in range(self.word_num):\n",
    "                self.P_w_T[i,k]=0\n",
    "                for j in range(self.doc_num):\n",
    "                    self.P_w_T[i,k]+=self.c_wd[i,j]*self.P_T_wd[k,i,j]\n",
    "            temp_sum = self.P_w_T[:,k].sum()\n",
    "            self.P_w_T[:,k]/=temp_sum\n",
    "\n",
    "        print(\" process P_T_d\")\n",
    "        for j in range(self.doc_num):\n",
    "            for k in range(self.topic_num):\n",
    "                self.P_T_d[k,j]=0\n",
    "                for i in range(self.word_num):\n",
    "                    self.P_T_d[k,j]+=self.c_wd[i,j]*self.P_T_wd[k,i,j]\n",
    "            self.P_w_T[:,j]/=self.doc_length[j]\n",
    "\n",
    "        print(\"...done\")\n",
    "\n",
    "    def iter(self):\n",
    "        self.E_step()\n",
    "        self.M_step()\n",
    "\n",
    "\n",
    "\n",
    "class PLSA:\n",
    "    def __init__(self,topic_num,alpha,beta):\n",
    "        self.topic_num=topic_num\n",
    "        self.param = parameter_retriever(topic_num)\n",
    "        \n",
    "        self.alpha=alpha\n",
    "        self.beta=beta\n",
    "    \n",
    "    def get_sim(self,idx_doc,q):\n",
    "        logsum=0\n",
    "        for q_word in get_query_word(q):\n",
    "            if q_word in self.param.index_term_list:\n",
    "                i=self.param.index_term_list.index(q_word)\n",
    "                first = self.alpha*self.param.c_wd[i,idx_doc]/self.param.doc_length[idx_doc]\n",
    "                second = 0\n",
    "                for k in range(self.topic_num):\n",
    "                    second+=self.param.P_w_T[i,k]*self.param.P_T_d[k,idx_doc]\n",
    "                second*=self.beta\n",
    "                third = (1-self.alpha-self.beta)*self.param.c_w[i]/self.param.doc_num\n",
    "                temp = np.logaddexp(first, second)\n",
    "                temp = np.logaddexp(temp, third)\n",
    "                logsum+=temp\n",
    "        return logsum\n",
    "\n",
    "\n",
    "    def query(self,q):\n",
    "        sim={}\n",
    "        for idx_doc,doc in enumerate(d_list):\n",
    "\n",
    "            sim[doc] = self.get_sim(idx_doc,q)\n",
    "        sim = sorted(sim.items(), key=lambda x:x[1],reverse=True)[:]\n",
    "        ans = \"\"\n",
    "        for i in sim:\n",
    "            ans+=i[0]+' '\n",
    "        return ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plsa=PLSA(6,10,0.75,0)\n",
    "f = open(\"ans.txt\",\"w\")\n",
    "f.write(\"Query,RetrievedDocuments\\n\")\n",
    "for q in q_list:\n",
    "    ranking=plsa.query(q)\n",
    "    f.writelines(q+\",\"+ranking+'\\n')\n",
    "f.close()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9fee4ced013193308f564d640256c01cc1027b2550c6e22c7b7e8d78aed89e33"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('ir': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
