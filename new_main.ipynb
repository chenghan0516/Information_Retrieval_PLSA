{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import math\n",
    "import numpy as np\n",
    "query_list_path = \"q_100_d_10000/query_list.txt\"\n",
    "doc_list_path = \"q_100_d_10000/doc_list.txt\"\n",
    "\n",
    "with open(query_list_path, \"r\") as f:\n",
    "    q_list = f.read().split('\\n')[:-1]\n",
    "with open(doc_list_path, \"r\") as f:\n",
    "    d_list = f.read().split('\\n')[:-1]\n",
    "d_dict = {doc:doc_index for doc_index,doc in enumerate(d_list)}\n",
    "\n",
    "def get_query_word(q):\n",
    "    with open(\"q_100_d_10000/queries/{}.txt\".format(q),'r') as f:\n",
    "        words = f.read().split(' ')\n",
    "    return words\n",
    "def get_doc_word(d):\n",
    "    with open(\"q_100_d_10000/docs/{}.txt\".format(d),'r') as f:\n",
    "        words = f.read().split(' ')\n",
    "    return words\n",
    "def get_random_probability_matrix(event_num,condition_num):\n",
    "    matrix=np.random.random_sample((event_num,condition_num))\n",
    "    for i in range(condition_num):\n",
    "        temp_sum = matrix[:,i].sum()\n",
    "        matrix[:,i]/=temp_sum #set sum to 1\n",
    "    return matrix\n",
    "\n",
    "class parameter_retriever:\n",
    "    def __init__(self,topic_num):\n",
    "        self.index_term_dict=dict()\n",
    "        self.index_term_num = 0\n",
    "        self.topic_num = topic_num\n",
    "        self.doc_num = len(d_list)\n",
    "\n",
    "        self.c_wd = [dict()]*self.index_term_num\n",
    "        self.doc_length = [0]*len(d_list)\n",
    "        self.c_w = [0]*self.index_term_num\n",
    "\n",
    "        self.P_w_T = []\n",
    "        self.P_T_d = []\n",
    "\n",
    "\n",
    "    def create_index_term_set(self):\n",
    "        print(\"creating index term set\")\n",
    "        print(\" creating index term set from query\")\n",
    "        index_term_set_q = set()\n",
    "        for q in tqdm(q_list):\n",
    "            words = get_query_word(q)\n",
    "            index_term_set_q = index_term_set_q.union(set(words))\n",
    "\n",
    "        print(\" creating index term set from doc\")\n",
    "        index_term_set_d = set()\n",
    "        for d in tqdm(d_list):\n",
    "            words = get_doc_word(d)\n",
    "            index_term_set_d = index_term_set_d.union(set(words))\n",
    "\n",
    "        index_term_set = index_term_set_q.union(index_term_set_d)\n",
    "        self.index_term_dict={index_term:index_term_index for index_term_index,index_term in enumerate(index_term_set)}\n",
    "        self.index_term_num = len(index_term_set)\n",
    "        print(\"number of words in index_term_set: {}\".format(self.index_term_num))\n",
    "        print(\"...done\")\n",
    "\n",
    "    def get_word_count_in_doc(self):\n",
    "        print(\"getting word counts in doc/doc_length/BG\")\n",
    "        self.c_wd = [dict() for _ in range(self.index_term_num)]\n",
    "        self.doc_length = [0]*len(d_list)\n",
    "        self.c_w = [0]*self.index_term_num\n",
    "\n",
    "        for doc_index,doc in tqdm(enumerate(d_list)):\n",
    "            words = get_doc_word(doc)\n",
    "            for word in set(words):\n",
    "                if word in self.index_term_dict:\n",
    "                    self.c_w[self.index_term_dict[word]]+=1\n",
    "                    self.c_wd[self.index_term_dict[word]][doc_index]=0\n",
    "            for word in words:\n",
    "                if word in self.index_term_dict:\n",
    "                    self.c_wd[self.index_term_dict[word]][doc_index]+=1\n",
    "                    self.doc_length[doc_index]+=1\n",
    "        print(\"...done\")\n",
    "\n",
    "    def clean_index_term_set(self):\n",
    "        print(\"cleaning index term whose c_w<=2\")\n",
    "        index_term_set = {index_term for index_term_index,index_term in enumerate(self.index_term_dict) if self.c_w[index_term_index]>2}\n",
    "        self.index_term_dict={index_term:index_term_index for index_term_index,index_term in enumerate(index_term_set)}\n",
    "        self.index_term_num = len(self.index_term_dict)\n",
    "        print(\" index_term_num after cleaning: {}\".format(self.index_term_num))\n",
    "        self.get_word_count_in_doc()\n",
    "        print(\"...done\")\n",
    "\n",
    "    def initPossibilities(self):\n",
    "        print(\"initializing possibilities\")\n",
    "        random_matrix = get_random_probability_matrix(self.index_term_num,self.topic_num)\n",
    "        for index_term_idx in range(self.index_term_num):\n",
    "            self.P_w_T.append([probability for probability in random_matrix[index_term_idx,:].tolist()])\n",
    "        \n",
    "        random_matrix = get_random_probability_matrix(self.topic_num,self.doc_num)\n",
    "        for topic_idx in range(self.topic_num):\n",
    "            self.P_T_d.append([probability for probability in random_matrix[topic_idx,:].tolist()])\n",
    "        print(\"...done\")\n",
    "\n",
    "\n",
    "    def calculate_P_T_wd(self,topic_idx,index_term_index,doc_index):\n",
    "        buffer = self.P_w_T[index_term_index][topic_idx]*self.P_T_d[topic_idx][doc_index]\n",
    "        sum=0\n",
    "        for k in range(self.topic_num):\n",
    "            sum+=self.P_w_T[index_term_index][k]*self.P_T_d[k][doc_index]\n",
    "        return buffer/sum\n",
    "        \n",
    "    def iter(self):\n",
    "        # print(\"start E_step\")\n",
    "        sum_of_cP_in_D=[[0]*self.topic_num]*self.index_term_num\n",
    "        sum_of_cP_in_V=[[0]*len(d_list)]*self.topic_num\n",
    "        for index_term_index in tqdm(range(self.index_term_num),leave=False):\n",
    "            for doc_index in self.c_wd[index_term_index].keys():\n",
    "                for topic_idx in range(self.topic_num):\n",
    "                    cP=self.c_wd[index_term_index][doc_index]*self.calculate_P_T_wd(topic_idx,index_term_index,doc_index)\n",
    "                    sum_of_cP_in_D[index_term_index][topic_idx]+=cP\n",
    "                    sum_of_cP_in_V[topic_idx][doc_index]+=cP\n",
    "        # print(\"start M_step\")\n",
    "        # print(\" process P_w_T\")\n",
    "        for topic_index in range(self.topic_num):\n",
    "            sum_of_topic_k = 0\n",
    "            for index_term_index in range(self.index_term_num):\n",
    "                sum_of_topic_k+=sum_of_cP_in_D[index_term_index][topic_index]\n",
    "            for index_term_index in range(self.index_term_num):\n",
    "                self.P_w_T[index_term_index][topic_index]=sum_of_cP_in_D[index_term_index][topic_index]/sum_of_topic_k\n",
    "        # print(\" process P_T_d\")\n",
    "        for doc_index in range(len(d_list)):\n",
    "            if self.doc_length[doc_index]==0:\n",
    "                for topic_index in range(self.topic_num):\n",
    "                    self.P_T_d[topic_index][doc_index] = 1/self.topic_num\n",
    "            else:\n",
    "                for topic_index in range(self.topic_num):\n",
    "                    self.P_T_d[topic_index][doc_index]=sum_of_cP_in_V[topic_index][doc_index]/self.doc_length[doc_index]\n",
    "        # print(\"...done\")\n",
    "\n",
    "\n",
    "\n",
    "class PLSA:\n",
    "    def __init__(self,topic_num,alpha,beta):\n",
    "        self.topic_num=topic_num\n",
    "        self.param = parameter_retriever(topic_num)\n",
    "        \n",
    "        self.alpha=alpha\n",
    "        self.beta=beta\n",
    "    \n",
    "    def init_param(self):\n",
    "        self.param.create_index_term_set()\n",
    "        self.param.get_word_count_in_doc()\n",
    "        self.param.clean_index_term_set()\n",
    "        self.param.initPossibilities()\n",
    "\n",
    "    def train(self,n):\n",
    "        for i in tqdm(range(n)):\n",
    "            # print(\"iter: {}\".format(i))\n",
    "            self.param.iter()\n",
    "\n",
    "    def get_sim(self,idx_doc,q):\n",
    "        if self.param.doc_length[idx_doc]==0:\n",
    "            return -999999999\n",
    "        logsum=0\n",
    "        for word in get_query_word(q):\n",
    "            if word in self.param.index_term_dict:\n",
    "                word_index=self.param.index_term_dict[word]\n",
    "                if idx_doc in self.param.c_wd[word_index]:\n",
    "                    first = self.alpha*self.param.c_wd[word_index][idx_doc]/self.param.doc_length[idx_doc]\n",
    "                    first = math.log(first)\n",
    "                else:\n",
    "                    first = -15\n",
    "\n",
    "                second = 0\n",
    "                for topic_idx in range(self.topic_num):\n",
    "                    second+=self.param.P_w_T[word_index][topic_idx]*self.param.P_T_d[topic_idx][idx_doc]\n",
    "                second*=self.beta\n",
    "                second = math.log(second)\n",
    "                \n",
    "                third = (1-self.alpha-self.beta)*self.param.c_w[word_index]/self.param.doc_num\n",
    "                third = math.log(third)\n",
    "\n",
    "                temp = np.logaddexp(first, second)\n",
    "                temp = np.logaddexp(temp, third)\n",
    "                logsum+=temp\n",
    "        return logsum\n",
    "    def query(self,q):\n",
    "        sim={}\n",
    "        for idx_doc,doc in enumerate(d_list):\n",
    "\n",
    "            sim[doc] = self.get_sim(idx_doc,q)\n",
    "        sim = sorted(sim.items(), key=lambda x:x[1],reverse=True)\n",
    "        ans = \"\"\n",
    "        for i in sim:\n",
    "            ans+=i[0]+' '\n",
    "        return ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating index term set\n",
      " creating index term set from query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 14283.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " creating index term set from doc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:47<00:00, 210.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words in index_term_set: 93065\n",
      "...done\n",
      "getting word counts in doc/doc_length/BG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:09, 1043.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...done\n",
      "cleaning index term whose c_w<=2\n",
      " index_term_num after cleaning: 31371\n",
      "getting word counts in doc/doc_length/BG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:09, 1063.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...done\n",
      "...done\n",
      "initializing possibilities\n",
      "...done\n"
     ]
    }
   ],
   "source": [
    "#time: < 1min15sec\n",
    "#creating index term set ~47sec\n",
    "#cleaning index term whose c_w<=2 <10sec\n",
    "plsa=PLSA(32,0.3,0.1)\n",
    "plsa.init_param()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [1:06:43<8:55:56, 361.31s/it]\n",
      "  6%|▋         | 1993/31371 [00:21<06:54, 70.85it/s] "
     ]
    }
   ],
   "source": [
    "# 1Epoch\n",
    "# total<2min45sec\n",
    "plsa.train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 19/100 [00:16<01:11,  1.13it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14036/1554764915.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Query,RetrievedDocuments\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mranking\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplsa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwritelines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mranking\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14036/55162900.py\u001b[0m in \u001b[0;36mquery\u001b[1;34m(self, q)\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0midx_doc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m             \u001b[0msim\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_sim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx_doc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m         \u001b[0msim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[0mans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14036/55162900.py\u001b[0m in \u001b[0;36mget_sim\u001b[1;34m(self, idx_doc, q)\u001b[0m\n\u001b[0;32m    160\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m999999999\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[0mlogsum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mget_query_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_term_dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m                 \u001b[0mword_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_term_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "f=open(\"ans.txt\",\"w\")\n",
    "f.write(\"Query,RetrievedDocuments\\n\")\n",
    "for q in tqdm(q_list):\n",
    "    ranking=plsa.query(q)\n",
    "    f.writelines(q+\",\"+ranking+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('230_10.pickle', 'wb') as f:\n",
    "    pickle.dump(plsa, f)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9fee4ced013193308f564d640256c01cc1027b2550c6e22c7b7e8d78aed89e33"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('ir': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
